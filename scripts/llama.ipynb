{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.api import API\n",
    "from src.dataloader import DataLoader\n",
    "from src.prompter import prompter_factory\n",
    "from src.scorer import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY = \"92a6ac4a8feb39c91b4a3f77219e9c452d927f5f4d543d5969cc11c210795719\"\n",
    "BASE_URL = \"https://api.together.xyz\"\n",
    "ALLOWED_SYMPTOMS = ['anxiety', 'concentration problems', 'constipation', 'cough',\n",
    "                    'diarrhea', 'fatigue', 'fever', 'headache', 'nausea', \n",
    "                    'numbness and tingling', 'pain', 'poor appetite', 'rash', \n",
    "                    'shortness of breath', 'trouble drinking fluids', 'vomiting', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = API(api_key=TOGETHER_API_KEY, base_url=BASE_URL)\n",
    "client = api.get_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/batch_1_gs.csv', 'data/batch_2_gs.csv', 'data/batch_3_gs.csv', 'data/batch_4_gs.csv', 'data/batch_5_gs.csv', 'data/batch_6_gs.csv', 'data/batch_7_gs.csv', 'data/batch_8_gs.csv', 'data/batch_9_gs.csv', 'data/batch_10_gs.csv']\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(path=\"data/\")\n",
    "print(dataloader.list_csv_files())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataloader.get_standardized_dataframe(context_col=\"Text Data\",\n",
    "                                           target_binary_col=\"symptom_status_gs\",\n",
    "                                           target_multilabel_col=\"symptom_detail_gs\",\n",
    "                                           keep_other_cols=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = prompter_factory(prompter_type=\"binary\",\n",
    "                            client=client,\n",
    "                            model=\"meta-llama/Llama-2-70b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Are any medical symptoms mentioned in the transcript\"\n",
    "context = \"i have a headache every time i see a cat and i hear voices that are not there\"\n",
    "print(prompter.generate_single(prompt=prompt, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<s>[INST] The following question requires you to output a single word. Your answer will have to be Yes or No. It is very important that you follow this instruction. Are any medical symptoms mentioned in the transcript? i have a headache every time i see a cat and i hear voices that are not there [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"\"\"\n",
    "        <<SYS>>\n",
    "        Analyze the text in the content and evaluate if there are any medical symptoms mentioned in the transcript for 'User'.\\n\n",
    "        And return your medical analysis only as : Yes or No. It is of the uttermost importance that your response is either Yes or No.\\n\n",
    "        <</SYS>>\n",
    "        [INST]\n",
    "        User:{context}\n",
    "        [/INST]\\n\n",
    "\n",
    "        Assistant:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAI' object has no attribute 'create_completion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-70b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                       prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m      3\u001b[0m                                       max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                       temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                       top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                       frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                       presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                       stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m                                       n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenAI' object has no attribute 'create_completion'"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": context}]\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    temperature=self.temperature,\n",
    "    tools=self.tool,\n",
    "    tool_choice=self.tool_choice,\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-70b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-evaluation/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    temperature=0,\n",
    "    prompt=input,\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " removing the wheel and caliper. Then, you remove the brake pads and any other components that need to be replaced. Next, you install the new brake pads and reassemble the brake caliper. Finally, you reinstall the wheel and test the brakes to make sure they\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "prompt = \"\"\"To change the brakes on your car, you start by\"\"\"\n",
    "\n",
    "output = client.completions.create(\n",
    "  prompt = prompt, \n",
    "  model = model, \n",
    "  max_tokens = 64,\n",
    "  temperature = 0,\n",
    "  top_p = 0.7,\n",
    "  #stop = [] # add any sequence you want to stop generating at. \n",
    ")\n",
    "\n",
    "# print generated text\n",
    "print(output.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
